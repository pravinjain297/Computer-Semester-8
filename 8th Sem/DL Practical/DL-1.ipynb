{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfb5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d1cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Boston-house-price-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0bc0371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273.0   \n",
       "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273.0   \n",
       "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273.0   \n",
       "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273.0   \n",
       "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bfa809b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0acc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf20de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.drop(['MEDV'],axis=1)\n",
    "df_y = df['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a4fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac11461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "lin_y_pred = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52d96fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  23.380836480270375\n",
      "Accuracy of model:  76.61916351972963 %\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, lin_y_pred)\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Accuracy of model: \", 100-mse,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afddd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using sequential model where layers are stacked one after another, \n",
    "#output of previous layer is given to as input to next layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "#in this function we will define our model such as how many layers are present\n",
    "def pricepredictionmodel():\n",
    "    model = Sequential()\n",
    "    #1st layer is dense layer which consists on 128 neurons, since it is 1st layer we need to define input_shape of our training data\n",
    "    model.add(Dense(128,activation='relu', input_shape=(x_train.shape)))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    #sufficient to identify the price\n",
    "    model.add(Dense(1)) #since output is real number we haven't used any activation fuction to get o/p in real no\n",
    "              \n",
    "    #now we will compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62e3d6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 404, 13) for input KerasTensor(type_spec=TensorSpec(shape=(None, 404, 13), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (4, 13).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 404, 13) for input KerasTensor(type_spec=TensorSpec(shape=(None, 404, 13), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (4, 13).\n",
      " 87/101 [========================>.....] - ETA: 0s - loss: 83.8324 - mae: 6.7884 WARNING:tensorflow:Model was constructed with shape (None, 404, 13) for input KerasTensor(type_spec=TensorSpec(shape=(None, 404, 13), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 13).\n",
      "101/101 [==============================] - 1s 3ms/step - loss: 88.3318 - mae: 6.9160 - val_loss: 87.9943 - val_mae: 6.4946\n",
      "Epoch 2/300\n",
      "101/101 [==============================] - 0s 919us/step - loss: 69.5491 - mae: 6.1570 - val_loss: 84.4553 - val_mae: 5.7153\n",
      "Epoch 3/300\n",
      "101/101 [==============================] - 0s 966us/step - loss: 63.9659 - mae: 5.7420 - val_loss: 66.2822 - val_mae: 5.6867\n",
      "Epoch 4/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 58.4733 - mae: 5.5020 - val_loss: 70.3632 - val_mae: 5.4316\n",
      "Epoch 5/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 49.5851 - mae: 4.9992 - val_loss: 59.3125 - val_mae: 5.3204\n",
      "Epoch 6/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 46.6627 - mae: 4.8495 - val_loss: 56.5791 - val_mae: 5.8999\n",
      "Epoch 7/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 41.7041 - mae: 4.6894 - val_loss: 75.1436 - val_mae: 7.8327\n",
      "Epoch 8/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 48.6217 - mae: 5.0307 - val_loss: 76.2659 - val_mae: 5.8303\n",
      "Epoch 9/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 42.7871 - mae: 4.7016 - val_loss: 61.7270 - val_mae: 6.8650\n",
      "Epoch 10/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 44.2104 - mae: 5.0257 - val_loss: 49.3938 - val_mae: 4.5430\n",
      "Epoch 11/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 37.2651 - mae: 4.4711 - val_loss: 46.9710 - val_mae: 4.5030\n",
      "Epoch 12/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 36.8009 - mae: 4.2443 - val_loss: 46.3206 - val_mae: 5.2782\n",
      "Epoch 13/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 34.7361 - mae: 4.3838 - val_loss: 48.9457 - val_mae: 5.9529\n",
      "Epoch 14/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 36.3004 - mae: 4.4874 - val_loss: 48.4840 - val_mae: 4.5393\n",
      "Epoch 15/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 41.8828 - mae: 4.7760 - val_loss: 39.9809 - val_mae: 4.6379\n",
      "Epoch 16/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 37.2111 - mae: 4.4013 - val_loss: 39.2454 - val_mae: 4.3238\n",
      "Epoch 17/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 37.8376 - mae: 4.4869 - val_loss: 45.1194 - val_mae: 4.4126\n",
      "Epoch 18/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 33.0683 - mae: 4.2523 - val_loss: 40.2845 - val_mae: 5.0836\n",
      "Epoch 19/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 34.6221 - mae: 4.3094 - val_loss: 34.8594 - val_mae: 4.0917\n",
      "Epoch 20/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 33.0112 - mae: 4.0491 - val_loss: 53.6058 - val_mae: 6.3833\n",
      "Epoch 21/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 31.9850 - mae: 4.1851 - val_loss: 35.4222 - val_mae: 3.9933\n",
      "Epoch 22/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 29.2553 - mae: 3.8628 - val_loss: 38.4392 - val_mae: 5.2294\n",
      "Epoch 23/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 31.0221 - mae: 4.0928 - val_loss: 30.4971 - val_mae: 4.2107\n",
      "Epoch 24/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 26.6812 - mae: 3.8372 - val_loss: 32.3664 - val_mae: 4.6892\n",
      "Epoch 25/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 24.0867 - mae: 3.6057 - val_loss: 31.6396 - val_mae: 3.7987\n",
      "Epoch 26/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 31.5753 - mae: 4.0010 - val_loss: 31.1230 - val_mae: 4.6037\n",
      "Epoch 27/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 31.1109 - mae: 4.1257 - val_loss: 32.8271 - val_mae: 4.7139\n",
      "Epoch 28/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 26.9330 - mae: 3.7797 - val_loss: 51.0945 - val_mae: 6.0566\n",
      "Epoch 29/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 30.3557 - mae: 4.1242 - val_loss: 26.7658 - val_mae: 4.0963\n",
      "Epoch 30/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 24.4989 - mae: 3.5693 - val_loss: 34.3283 - val_mae: 5.0408\n",
      "Epoch 31/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 23.3032 - mae: 3.4899 - val_loss: 26.5902 - val_mae: 4.0428\n",
      "Epoch 32/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 23.6808 - mae: 3.5499 - val_loss: 31.0415 - val_mae: 4.4034\n",
      "Epoch 33/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 23.6235 - mae: 3.5610 - val_loss: 25.3016 - val_mae: 3.6377\n",
      "Epoch 34/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 25.5185 - mae: 3.5892 - val_loss: 22.4395 - val_mae: 3.5311\n",
      "Epoch 35/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.8372 - mae: 3.5010 - val_loss: 20.8687 - val_mae: 3.5022\n",
      "Epoch 36/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.7283 - mae: 3.4159 - val_loss: 23.1689 - val_mae: 3.6246\n",
      "Epoch 37/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 24.5012 - mae: 3.5884 - val_loss: 26.7984 - val_mae: 4.0853\n",
      "Epoch 38/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 27.4616 - mae: 3.7946 - val_loss: 22.9285 - val_mae: 3.3584\n",
      "Epoch 39/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 23.7641 - mae: 3.5216 - val_loss: 36.8644 - val_mae: 5.0468\n",
      "Epoch 40/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 27.5107 - mae: 3.8405 - val_loss: 62.1275 - val_mae: 5.4256\n",
      "Epoch 41/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 26.2545 - mae: 3.7787 - val_loss: 21.6094 - val_mae: 3.4347\n",
      "Epoch 42/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.3734 - mae: 3.3990 - val_loss: 18.3345 - val_mae: 3.2123\n",
      "Epoch 43/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.2886 - mae: 3.3896 - val_loss: 25.4844 - val_mae: 4.0348\n",
      "Epoch 44/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 21.4246 - mae: 3.3739 - val_loss: 30.1935 - val_mae: 3.8795\n",
      "Epoch 45/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 21.9301 - mae: 3.3459 - val_loss: 19.2436 - val_mae: 3.1941\n",
      "Epoch 46/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 24.7209 - mae: 3.6687 - val_loss: 23.9665 - val_mae: 3.9640\n",
      "Epoch 47/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 25.3489 - mae: 3.6228 - val_loss: 19.1792 - val_mae: 3.1657\n",
      "Epoch 48/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 21.0372 - mae: 3.3201 - val_loss: 51.1416 - val_mae: 5.2959\n",
      "Epoch 49/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 27.1103 - mae: 3.7065 - val_loss: 22.5303 - val_mae: 3.6995\n",
      "Epoch 50/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 23.1814 - mae: 3.5174 - val_loss: 25.5477 - val_mae: 3.6477\n",
      "Epoch 51/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.9653 - mae: 3.2456 - val_loss: 25.4327 - val_mae: 3.4556\n",
      "Epoch 52/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.2404 - mae: 3.4529 - val_loss: 25.2499 - val_mae: 3.4948\n",
      "Epoch 53/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.9046 - mae: 3.3762 - val_loss: 19.1294 - val_mae: 3.1587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.0052 - mae: 3.4635 - val_loss: 22.0086 - val_mae: 3.5233\n",
      "Epoch 55/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 23.3116 - mae: 3.4646 - val_loss: 21.3839 - val_mae: 3.5770\n",
      "Epoch 56/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.8942 - mae: 3.3271 - val_loss: 21.0426 - val_mae: 3.4743\n",
      "Epoch 57/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.1203 - mae: 3.2732 - val_loss: 22.8460 - val_mae: 3.3144\n",
      "Epoch 58/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 22.0385 - mae: 3.3577 - val_loss: 16.4496 - val_mae: 3.0477\n",
      "Epoch 59/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 26.0582 - mae: 3.6811 - val_loss: 19.2848 - val_mae: 3.2556\n",
      "Epoch 60/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.9579 - mae: 3.0467 - val_loss: 28.4129 - val_mae: 4.3895\n",
      "Epoch 61/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.2983 - mae: 3.0808 - val_loss: 17.7237 - val_mae: 3.0402\n",
      "Epoch 62/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 21.3215 - mae: 3.2631 - val_loss: 21.8055 - val_mae: 3.7175\n",
      "Epoch 63/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.0259 - mae: 3.3570 - val_loss: 18.6389 - val_mae: 3.1530\n",
      "Epoch 64/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.3598 - mae: 3.2125 - val_loss: 19.3859 - val_mae: 3.3743\n",
      "Epoch 65/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 22.6561 - mae: 3.4197 - val_loss: 20.0597 - val_mae: 3.4802\n",
      "Epoch 66/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 21.5911 - mae: 3.1734 - val_loss: 18.4885 - val_mae: 3.3577\n",
      "Epoch 67/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.9118 - mae: 3.3802 - val_loss: 18.6873 - val_mae: 3.2125\n",
      "Epoch 68/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.0399 - mae: 3.1281 - val_loss: 21.1101 - val_mae: 3.5595\n",
      "Epoch 69/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.6390 - mae: 3.0724 - val_loss: 17.6393 - val_mae: 3.1555\n",
      "Epoch 70/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.5764 - mae: 3.0361 - val_loss: 19.6727 - val_mae: 3.1628\n",
      "Epoch 71/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.7894 - mae: 3.2963 - val_loss: 19.7672 - val_mae: 3.3909\n",
      "Epoch 72/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.5581 - mae: 3.1192 - val_loss: 19.4083 - val_mae: 3.1543\n",
      "Epoch 73/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.6325 - mae: 3.0828 - val_loss: 25.6167 - val_mae: 4.1731\n",
      "Epoch 74/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.9306 - mae: 3.2208 - val_loss: 18.4598 - val_mae: 3.1040\n",
      "Epoch 75/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.6111 - mae: 3.1618 - val_loss: 21.2663 - val_mae: 3.7439\n",
      "Epoch 76/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.7483 - mae: 3.0529 - val_loss: 21.9717 - val_mae: 3.6075\n",
      "Epoch 77/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.5737 - mae: 3.1108 - val_loss: 24.9441 - val_mae: 3.3996\n",
      "Epoch 78/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.9901 - mae: 3.2170 - val_loss: 17.6410 - val_mae: 3.1509\n",
      "Epoch 79/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.2873 - mae: 3.1954 - val_loss: 19.0440 - val_mae: 3.2305\n",
      "Epoch 80/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.9105 - mae: 3.2024 - val_loss: 23.4750 - val_mae: 3.3091\n",
      "Epoch 81/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.9451 - mae: 3.0966 - val_loss: 17.6886 - val_mae: 3.1195\n",
      "Epoch 82/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.1271 - mae: 3.1503 - val_loss: 21.7546 - val_mae: 3.2521\n",
      "Epoch 83/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.3392 - mae: 2.9588 - val_loss: 17.9743 - val_mae: 3.0290\n",
      "Epoch 84/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.9187 - mae: 2.8960 - val_loss: 18.4162 - val_mae: 3.1690\n",
      "Epoch 85/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 15.8628 - mae: 2.8249 - val_loss: 27.6270 - val_mae: 3.4878\n",
      "Epoch 86/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.1268 - mae: 2.9514 - val_loss: 21.0953 - val_mae: 3.1621\n",
      "Epoch 87/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.6545 - mae: 3.2009 - val_loss: 16.8108 - val_mae: 2.9893\n",
      "Epoch 88/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 16.3615 - mae: 2.8334 - val_loss: 20.2536 - val_mae: 3.3334\n",
      "Epoch 89/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.5892 - mae: 3.0923 - val_loss: 15.9634 - val_mae: 2.9487\n",
      "Epoch 90/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.7965 - mae: 3.0293 - val_loss: 15.9281 - val_mae: 2.9690\n",
      "Epoch 91/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.0667 - mae: 2.9158 - val_loss: 16.6127 - val_mae: 2.9829\n",
      "Epoch 92/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.6040 - mae: 2.9679 - val_loss: 18.3561 - val_mae: 3.2898\n",
      "Epoch 93/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.6703 - mae: 3.0745 - val_loss: 18.7509 - val_mae: 3.0658\n",
      "Epoch 94/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.8994 - mae: 2.8610 - val_loss: 25.6646 - val_mae: 4.0938\n",
      "Epoch 95/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.7467 - mae: 2.7651 - val_loss: 16.2704 - val_mae: 3.0510\n",
      "Epoch 96/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.4686 - mae: 2.9064 - val_loss: 16.7003 - val_mae: 3.0684\n",
      "Epoch 97/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.7555 - mae: 2.9261 - val_loss: 17.2646 - val_mae: 2.8989\n",
      "Epoch 98/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.3989 - mae: 2.9440 - val_loss: 19.7956 - val_mae: 3.5246\n",
      "Epoch 99/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.5272 - mae: 2.8816 - val_loss: 17.7906 - val_mae: 3.0164\n",
      "Epoch 100/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 20.9894 - mae: 3.2434 - val_loss: 21.9278 - val_mae: 3.7323\n",
      "Epoch 101/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.0829 - mae: 3.2099 - val_loss: 19.6139 - val_mae: 3.4128\n",
      "Epoch 102/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.4498 - mae: 2.7180 - val_loss: 27.2933 - val_mae: 4.2598\n",
      "Epoch 103/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.2878 - mae: 3.0264 - val_loss: 16.6118 - val_mae: 3.0220\n",
      "Epoch 104/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.7727 - mae: 2.8091 - val_loss: 19.6830 - val_mae: 3.3974\n",
      "Epoch 105/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.5975 - mae: 3.0691 - val_loss: 18.4228 - val_mae: 3.3509\n",
      "Epoch 106/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 14.7889 - mae: 2.8177 - val_loss: 19.6538 - val_mae: 3.1267\n",
      "Epoch 107/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.8530 - mae: 2.9894 - val_loss: 23.4802 - val_mae: 3.8069\n",
      "Epoch 108/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.7548 - mae: 2.7394 - val_loss: 17.5102 - val_mae: 3.2021\n",
      "Epoch 109/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.1612 - mae: 2.8706 - val_loss: 15.8968 - val_mae: 2.9101\n",
      "Epoch 110/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.6075 - mae: 2.6647 - val_loss: 16.7592 - val_mae: 2.9630\n",
      "Epoch 111/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.8661 - mae: 2.8748 - val_loss: 27.9303 - val_mae: 3.5309\n",
      "Epoch 112/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 19.1508 - mae: 3.0088 - val_loss: 19.5680 - val_mae: 3.0450\n",
      "Epoch 113/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.8427 - mae: 2.7231 - val_loss: 26.7082 - val_mae: 4.2175\n",
      "Epoch 114/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step - loss: 17.5074 - mae: 2.8663 - val_loss: 21.4611 - val_mae: 3.2507\n",
      "Epoch 115/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.2895 - mae: 2.9297 - val_loss: 18.1301 - val_mae: 3.4042\n",
      "Epoch 116/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.0516 - mae: 2.7840 - val_loss: 15.5820 - val_mae: 3.0083\n",
      "Epoch 117/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.7317 - mae: 2.8255 - val_loss: 19.4931 - val_mae: 3.5010\n",
      "Epoch 118/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.1961 - mae: 3.0021 - val_loss: 21.4471 - val_mae: 3.5165\n",
      "Epoch 119/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.1165 - mae: 2.6282 - val_loss: 16.7594 - val_mae: 3.2118\n",
      "Epoch 120/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.3740 - mae: 3.0569 - val_loss: 17.1184 - val_mae: 3.1531\n",
      "Epoch 121/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.7585 - mae: 2.8322 - val_loss: 18.1325 - val_mae: 3.2372\n",
      "Epoch 122/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 18.9807 - mae: 3.0954 - val_loss: 16.3641 - val_mae: 2.8633\n",
      "Epoch 123/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.3202 - mae: 2.8069 - val_loss: 16.7922 - val_mae: 3.1574\n",
      "Epoch 124/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.5099 - mae: 2.7747 - val_loss: 16.9750 - val_mae: 3.1622\n",
      "Epoch 125/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.6043 - mae: 2.8795 - val_loss: 24.5124 - val_mae: 3.9514\n",
      "Epoch 126/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.9405 - mae: 2.8006 - val_loss: 17.2395 - val_mae: 3.0493\n",
      "Epoch 127/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.7350 - mae: 2.7374 - val_loss: 15.7399 - val_mae: 2.9077\n",
      "Epoch 128/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.3257 - mae: 2.8304 - val_loss: 16.8867 - val_mae: 2.7931\n",
      "Epoch 129/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.7017 - mae: 2.6416 - val_loss: 14.9247 - val_mae: 2.8889\n",
      "Epoch 130/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.8630 - mae: 2.7208 - val_loss: 32.1491 - val_mae: 3.8477\n",
      "Epoch 131/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 16.3288 - mae: 2.8404 - val_loss: 17.1409 - val_mae: 3.1712\n",
      "Epoch 132/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 17.7962 - mae: 2.9432 - val_loss: 20.7182 - val_mae: 3.6109\n",
      "Epoch 133/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.8443 - mae: 2.8552 - val_loss: 16.0960 - val_mae: 2.9934\n",
      "Epoch 134/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.2228 - mae: 2.7108 - val_loss: 16.4014 - val_mae: 2.9135\n",
      "Epoch 135/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.2945 - mae: 2.5386 - val_loss: 16.5983 - val_mae: 3.0686\n",
      "Epoch 136/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 13.2411 - mae: 2.5592 - val_loss: 15.3360 - val_mae: 2.9508\n",
      "Epoch 137/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 14.1449 - mae: 2.5940 - val_loss: 17.5444 - val_mae: 3.2317\n",
      "Epoch 138/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 15.0770 - mae: 2.6473 - val_loss: 17.4244 - val_mae: 3.1746\n",
      "Epoch 139/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 13.9641 - mae: 2.6226 - val_loss: 16.7301 - val_mae: 2.9514\n",
      "Epoch 140/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.5537 - mae: 2.4201 - val_loss: 17.3572 - val_mae: 3.2119\n",
      "Epoch 141/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.5124 - mae: 2.7888 - val_loss: 16.6107 - val_mae: 3.0249\n",
      "Epoch 142/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.6117 - mae: 2.8298 - val_loss: 16.2962 - val_mae: 3.0155\n",
      "Epoch 143/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.8379 - mae: 2.5675 - val_loss: 18.3336 - val_mae: 3.1038\n",
      "Epoch 144/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.6768 - mae: 2.6121 - val_loss: 15.7123 - val_mae: 2.9491\n",
      "Epoch 145/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.1116 - mae: 2.4982 - val_loss: 16.9004 - val_mae: 3.0522\n",
      "Epoch 146/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.5895 - mae: 2.6798 - val_loss: 18.5290 - val_mae: 3.1980\n",
      "Epoch 147/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.2608 - mae: 2.6060 - val_loss: 19.8485 - val_mae: 3.5674\n",
      "Epoch 148/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.3705 - mae: 2.6960 - val_loss: 16.0071 - val_mae: 3.1484\n",
      "Epoch 149/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.0848 - mae: 2.4614 - val_loss: 16.1026 - val_mae: 2.9712\n",
      "Epoch 150/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.9619 - mae: 2.6265 - val_loss: 16.1759 - val_mae: 3.0871\n",
      "Epoch 151/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.9786 - mae: 2.4601 - val_loss: 16.8428 - val_mae: 2.9544\n",
      "Epoch 152/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.8669 - mae: 2.4677 - val_loss: 16.9400 - val_mae: 3.2603\n",
      "Epoch 153/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.2647 - mae: 2.8586 - val_loss: 19.1295 - val_mae: 3.4350\n",
      "Epoch 154/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 16.4944 - mae: 2.8990 - val_loss: 19.5981 - val_mae: 3.4351\n",
      "Epoch 155/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 12.1352 - mae: 2.4169 - val_loss: 16.9166 - val_mae: 3.2072\n",
      "Epoch 156/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.1749 - mae: 2.4581 - val_loss: 26.1019 - val_mae: 3.5791\n",
      "Epoch 157/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.6078 - mae: 2.8086 - val_loss: 14.3437 - val_mae: 2.7923\n",
      "Epoch 158/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.2597 - mae: 2.5254 - val_loss: 17.9885 - val_mae: 2.9846\n",
      "Epoch 159/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.4982 - mae: 2.6723 - val_loss: 15.7116 - val_mae: 2.9376\n",
      "Epoch 160/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.2341 - mae: 2.5387 - val_loss: 19.0971 - val_mae: 2.9959\n",
      "Epoch 161/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1387 - mae: 2.3796 - val_loss: 18.2586 - val_mae: 3.1062\n",
      "Epoch 162/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.7140 - mae: 2.7599 - val_loss: 16.9614 - val_mae: 2.9151\n",
      "Epoch 163/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.2444 - mae: 2.6278 - val_loss: 16.7906 - val_mae: 3.1369\n",
      "Epoch 164/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.6059 - mae: 2.5288 - val_loss: 18.5112 - val_mae: 3.4292\n",
      "Epoch 165/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.5744 - mae: 2.7133 - val_loss: 16.4629 - val_mae: 2.9073\n",
      "Epoch 166/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 13.4123 - mae: 2.5503 - val_loss: 16.4231 - val_mae: 3.0335\n",
      "Epoch 167/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.9952 - mae: 2.5856 - val_loss: 16.3796 - val_mae: 3.0952\n",
      "Epoch 168/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.6344 - mae: 2.5921 - val_loss: 19.7695 - val_mae: 3.4797\n",
      "Epoch 169/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 17.4181 - mae: 2.9665 - val_loss: 17.4477 - val_mae: 2.9474\n",
      "Epoch 170/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.8576 - mae: 2.5769 - val_loss: 17.9560 - val_mae: 2.9647\n",
      "Epoch 171/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.1135 - mae: 2.6469 - val_loss: 21.6464 - val_mae: 3.2265\n",
      "Epoch 172/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 14.9182 - mae: 2.6749 - val_loss: 20.6164 - val_mae: 3.3337\n",
      "Epoch 173/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.5594 - mae: 2.6401 - val_loss: 17.4587 - val_mae: 3.3217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.0188 - mae: 2.6870 - val_loss: 19.0595 - val_mae: 3.4656\n",
      "Epoch 175/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.9944 - mae: 2.5718 - val_loss: 19.8517 - val_mae: 3.5329\n",
      "Epoch 176/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.5714 - mae: 2.5597 - val_loss: 15.9709 - val_mae: 2.9768\n",
      "Epoch 177/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.4171 - mae: 2.4818 - val_loss: 16.2606 - val_mae: 3.0154\n",
      "Epoch 178/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.9695 - mae: 2.5032 - val_loss: 29.0537 - val_mae: 4.2293\n",
      "Epoch 179/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.6525 - mae: 2.4366 - val_loss: 18.6378 - val_mae: 3.3620\n",
      "Epoch 180/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.1628 - mae: 2.4258 - val_loss: 21.5355 - val_mae: 3.6860\n",
      "Epoch 181/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.2798 - mae: 2.3610 - val_loss: 20.4155 - val_mae: 3.2533\n",
      "Epoch 182/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.8452 - mae: 2.8822 - val_loss: 22.5989 - val_mae: 3.3964\n",
      "Epoch 183/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 13.0282 - mae: 2.5556 - val_loss: 16.5092 - val_mae: 3.1496\n",
      "Epoch 184/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 13.0667 - mae: 2.4710 - val_loss: 20.1527 - val_mae: 3.3268\n",
      "Epoch 185/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.9433 - mae: 2.6313 - val_loss: 22.3708 - val_mae: 3.5451\n",
      "Epoch 186/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 16.5874 - mae: 2.7740 - val_loss: 19.6944 - val_mae: 3.1842\n",
      "Epoch 187/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1614 - mae: 2.4813 - val_loss: 13.5677 - val_mae: 2.7915\n",
      "Epoch 188/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 12.5475 - mae: 2.4260 - val_loss: 20.9593 - val_mae: 3.0721\n",
      "Epoch 189/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 13.2418 - mae: 2.5768 - val_loss: 21.4420 - val_mae: 3.6334\n",
      "Epoch 190/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 11.9543 - mae: 2.4473 - val_loss: 15.0357 - val_mae: 2.8576\n",
      "Epoch 191/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.0204 - mae: 2.2551 - val_loss: 14.8895 - val_mae: 2.9528\n",
      "Epoch 192/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.0835 - mae: 2.4484 - val_loss: 15.3276 - val_mae: 2.9604\n",
      "Epoch 193/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.3529 - mae: 2.4144 - val_loss: 16.5211 - val_mae: 3.0450\n",
      "Epoch 194/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.5962 - mae: 2.4400 - val_loss: 15.1074 - val_mae: 2.9049\n",
      "Epoch 195/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1728 - mae: 2.3762 - val_loss: 15.7716 - val_mae: 3.0523\n",
      "Epoch 196/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.6064 - mae: 2.4579 - val_loss: 15.7848 - val_mae: 2.7666\n",
      "Epoch 197/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.5620 - mae: 2.4258 - val_loss: 14.1555 - val_mae: 2.8122\n",
      "Epoch 198/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.0036 - mae: 2.2635 - val_loss: 16.5387 - val_mae: 2.9920\n",
      "Epoch 199/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.6745 - mae: 2.3538 - val_loss: 21.3504 - val_mae: 3.5062\n",
      "Epoch 200/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.7124 - mae: 2.2942 - val_loss: 15.7268 - val_mae: 2.9010\n",
      "Epoch 201/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.8645 - mae: 2.5287 - val_loss: 16.0353 - val_mae: 3.1059\n",
      "Epoch 202/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.8373 - mae: 2.5215 - val_loss: 15.5350 - val_mae: 2.9288\n",
      "Epoch 203/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.8431 - mae: 2.5165 - val_loss: 20.7481 - val_mae: 3.3311\n",
      "Epoch 204/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.7288 - mae: 2.3959 - val_loss: 16.9562 - val_mae: 3.0833\n",
      "Epoch 205/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.9804 - mae: 2.4884 - val_loss: 18.0856 - val_mae: 3.2590\n",
      "Epoch 206/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.5897 - mae: 2.5707 - val_loss: 15.7608 - val_mae: 2.8964\n",
      "Epoch 207/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.9490 - mae: 2.3937 - val_loss: 31.5212 - val_mae: 4.5696\n",
      "Epoch 208/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.1590 - mae: 2.5119 - val_loss: 22.9589 - val_mae: 3.4246\n",
      "Epoch 209/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.0025 - mae: 2.7371 - val_loss: 16.7342 - val_mae: 2.9614\n",
      "Epoch 210/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.3795 - mae: 2.4663 - val_loss: 16.0394 - val_mae: 2.8573\n",
      "Epoch 211/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.3797 - mae: 2.7284 - val_loss: 17.7973 - val_mae: 3.0538\n",
      "Epoch 212/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.4220 - mae: 2.3392 - val_loss: 15.9135 - val_mae: 3.0077\n",
      "Epoch 213/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.5590 - mae: 2.4085 - val_loss: 15.1492 - val_mae: 2.8278\n",
      "Epoch 214/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.9554 - mae: 2.4362 - val_loss: 16.2712 - val_mae: 2.9814\n",
      "Epoch 215/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.3186 - mae: 2.5373 - val_loss: 21.1220 - val_mae: 3.7997\n",
      "Epoch 216/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.0512 - mae: 2.4800 - val_loss: 16.9598 - val_mae: 2.9835\n",
      "Epoch 217/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.7860 - mae: 2.4336 - val_loss: 20.3510 - val_mae: 3.5453\n",
      "Epoch 218/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.9548 - mae: 2.6365 - val_loss: 14.4993 - val_mae: 2.8381\n",
      "Epoch 219/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.9751 - mae: 2.2932 - val_loss: 15.5110 - val_mae: 2.8976\n",
      "Epoch 220/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.9421 - mae: 2.2384 - val_loss: 15.1140 - val_mae: 2.9636\n",
      "Epoch 221/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.5594 - mae: 2.3121 - val_loss: 13.9023 - val_mae: 2.8815\n",
      "Epoch 222/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.2234 - mae: 2.3148 - val_loss: 16.0731 - val_mae: 3.0986\n",
      "Epoch 223/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.4020 - mae: 2.4936 - val_loss: 16.5244 - val_mae: 2.8883\n",
      "Epoch 224/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.1750 - mae: 2.5546 - val_loss: 13.3710 - val_mae: 2.7362\n",
      "Epoch 225/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.9498 - mae: 2.2366 - val_loss: 18.9249 - val_mae: 3.3144\n",
      "Epoch 226/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.4480 - mae: 2.1164 - val_loss: 14.5612 - val_mae: 2.8280\n",
      "Epoch 227/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.7376 - mae: 2.3295 - val_loss: 14.4697 - val_mae: 2.8308\n",
      "Epoch 228/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.9820 - mae: 2.3204 - val_loss: 29.8779 - val_mae: 3.9500\n",
      "Epoch 229/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.9476 - mae: 2.6778 - val_loss: 16.5001 - val_mae: 3.1524\n",
      "Epoch 230/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.5231 - mae: 2.1910 - val_loss: 15.6854 - val_mae: 2.9355\n",
      "Epoch 231/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.7063 - mae: 2.5284 - val_loss: 13.7093 - val_mae: 2.7003\n",
      "Epoch 232/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.2095 - mae: 2.3517 - val_loss: 15.4305 - val_mae: 2.9498\n",
      "Epoch 233/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1416 - mae: 2.3458 - val_loss: 16.7102 - val_mae: 2.9823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.8129 - mae: 2.2942 - val_loss: 14.1872 - val_mae: 2.7913\n",
      "Epoch 235/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.3249 - mae: 2.4128 - val_loss: 16.3290 - val_mae: 3.2295\n",
      "Epoch 236/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.3578 - mae: 2.3439 - val_loss: 16.1039 - val_mae: 2.8891\n",
      "Epoch 237/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.1813 - mae: 2.2604 - val_loss: 16.1501 - val_mae: 2.8388\n",
      "Epoch 238/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.9000 - mae: 2.4936 - val_loss: 15.8973 - val_mae: 3.1235\n",
      "Epoch 239/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.6860 - mae: 2.1282 - val_loss: 14.1315 - val_mae: 2.7644\n",
      "Epoch 240/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.8426 - mae: 2.2466 - val_loss: 16.6837 - val_mae: 3.1939\n",
      "Epoch 241/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.7436 - mae: 2.4737 - val_loss: 15.9367 - val_mae: 2.8657\n",
      "Epoch 242/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1874 - mae: 2.4464 - val_loss: 15.9398 - val_mae: 2.9048\n",
      "Epoch 243/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.9024 - mae: 2.5951 - val_loss: 15.7506 - val_mae: 3.0437\n",
      "Epoch 244/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.3965 - mae: 2.2906 - val_loss: 27.5944 - val_mae: 3.4960\n",
      "Epoch 245/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.5390 - mae: 2.5833 - val_loss: 17.4120 - val_mae: 3.1832\n",
      "Epoch 246/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.1147 - mae: 2.2784 - val_loss: 14.0158 - val_mae: 2.7627\n",
      "Epoch 247/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.2489 - mae: 2.2961 - val_loss: 18.3383 - val_mae: 3.3777\n",
      "Epoch 248/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.2832 - mae: 2.4567 - val_loss: 15.3190 - val_mae: 2.7628\n",
      "Epoch 249/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.7030 - mae: 2.1646 - val_loss: 16.4221 - val_mae: 2.8729\n",
      "Epoch 250/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.9644 - mae: 2.0963 - val_loss: 18.6706 - val_mae: 3.0265\n",
      "Epoch 251/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.5105 - mae: 2.2666 - val_loss: 14.3611 - val_mae: 2.7937\n",
      "Epoch 252/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1147 - mae: 2.3953 - val_loss: 16.0951 - val_mae: 2.8543\n",
      "Epoch 253/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.8459 - mae: 2.1153 - val_loss: 13.5628 - val_mae: 2.7871\n",
      "Epoch 254/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.7634 - mae: 2.1845 - val_loss: 14.4373 - val_mae: 2.7975\n",
      "Epoch 255/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.5671 - mae: 2.2828 - val_loss: 21.1991 - val_mae: 3.7808\n",
      "Epoch 256/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.4279 - mae: 2.4029 - val_loss: 16.6942 - val_mae: 2.9189\n",
      "Epoch 257/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.1406 - mae: 2.6493 - val_loss: 15.8112 - val_mae: 3.0645\n",
      "Epoch 258/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.9736 - mae: 2.2456 - val_loss: 15.7380 - val_mae: 2.9116\n",
      "Epoch 259/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.5282 - mae: 2.2531 - val_loss: 14.2200 - val_mae: 2.6931\n",
      "Epoch 260/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.9177 - mae: 2.1825 - val_loss: 16.1777 - val_mae: 2.9681\n",
      "Epoch 261/300\n",
      "101/101 [==============================] - 0s 1ms/step - loss: 11.1971 - mae: 2.2952 - val_loss: 16.3400 - val_mae: 3.0958\n",
      "Epoch 262/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 7.8883 - mae: 1.9992 - val_loss: 17.3640 - val_mae: 3.0708\n",
      "Epoch 263/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.6545 - mae: 2.1806 - val_loss: 17.2456 - val_mae: 3.1442\n",
      "Epoch 264/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 10.2383 - mae: 2.3122 - val_loss: 20.2850 - val_mae: 3.5557\n",
      "Epoch 265/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.9975 - mae: 2.0878 - val_loss: 22.4307 - val_mae: 3.5330\n",
      "Epoch 266/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 14.7334 - mae: 2.6979 - val_loss: 13.1433 - val_mae: 2.6349\n",
      "Epoch 267/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.4105 - mae: 2.2929 - val_loss: 14.6151 - val_mae: 2.8723\n",
      "Epoch 268/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.3361 - mae: 2.1132 - val_loss: 13.8489 - val_mae: 2.7442\n",
      "Epoch 269/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.5502 - mae: 2.1344 - val_loss: 14.7656 - val_mae: 2.8849\n",
      "Epoch 270/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 13.4803 - mae: 2.5915 - val_loss: 17.7048 - val_mae: 2.9588\n",
      "Epoch 271/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.7138 - mae: 2.1249 - val_loss: 14.7913 - val_mae: 2.6961\n",
      "Epoch 272/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.8683 - mae: 2.1880 - val_loss: 18.3435 - val_mae: 3.0285\n",
      "Epoch 273/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.1799 - mae: 2.3292 - val_loss: 22.5567 - val_mae: 3.4639\n",
      "Epoch 274/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.3719 - mae: 2.2024 - val_loss: 17.4518 - val_mae: 3.0150\n",
      "Epoch 275/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.8638 - mae: 2.1732 - val_loss: 16.5058 - val_mae: 3.0264\n",
      "Epoch 276/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.8201 - mae: 2.1533 - val_loss: 16.9409 - val_mae: 3.0102\n",
      "Epoch 277/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.9795 - mae: 2.1904 - val_loss: 15.2028 - val_mae: 2.9048\n",
      "Epoch 278/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.5178 - mae: 2.2981 - val_loss: 20.1535 - val_mae: 3.4007\n",
      "Epoch 279/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.1486 - mae: 2.1875 - val_loss: 13.7332 - val_mae: 2.8829\n",
      "Epoch 280/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 7.9678 - mae: 2.0088 - val_loss: 16.6744 - val_mae: 2.9311\n",
      "Epoch 281/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.2696 - mae: 2.3270 - val_loss: 13.4194 - val_mae: 2.7495\n",
      "Epoch 282/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.0830 - mae: 2.1855 - val_loss: 18.9628 - val_mae: 3.2966\n",
      "Epoch 283/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 12.7072 - mae: 2.5442 - val_loss: 15.8478 - val_mae: 2.8826\n",
      "Epoch 284/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.3287 - mae: 2.1906 - val_loss: 19.3901 - val_mae: 3.1240\n",
      "Epoch 285/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.3426 - mae: 2.4487 - val_loss: 14.1296 - val_mae: 2.7862\n",
      "Epoch 286/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.8994 - mae: 2.1739 - val_loss: 23.5689 - val_mae: 3.5766\n",
      "Epoch 287/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.4518 - mae: 2.3937 - val_loss: 13.1138 - val_mae: 2.7980\n",
      "Epoch 288/300\n",
      "101/101 [==============================] - 0s 3ms/step - loss: 10.3639 - mae: 2.2768 - val_loss: 14.0023 - val_mae: 2.7654\n",
      "Epoch 289/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.3957 - mae: 2.1822 - val_loss: 16.3085 - val_mae: 2.9312\n",
      "Epoch 290/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.7129 - mae: 2.1423 - val_loss: 15.6083 - val_mae: 3.1016\n",
      "Epoch 291/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.4358 - mae: 2.0922 - val_loss: 15.5952 - val_mae: 3.0716\n",
      "Epoch 292/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 15.0637 - mae: 2.7389 - val_loss: 24.5570 - val_mae: 3.8586\n",
      "Epoch 293/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.2413 - mae: 2.3209 - val_loss: 13.1106 - val_mae: 2.7703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 11.2032 - mae: 2.3256 - val_loss: 16.5713 - val_mae: 3.0004\n",
      "Epoch 295/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.1825 - mae: 2.2404 - val_loss: 16.5751 - val_mae: 2.9474\n",
      "Epoch 296/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.7249 - mae: 2.2000 - val_loss: 15.0008 - val_mae: 2.7895\n",
      "Epoch 297/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 10.0870 - mae: 2.1723 - val_loss: 14.8521 - val_mae: 2.9723\n",
      "Epoch 298/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.9995 - mae: 2.2090 - val_loss: 19.0184 - val_mae: 3.2637\n",
      "Epoch 299/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 8.7570 - mae: 2.1604 - val_loss: 15.5748 - val_mae: 2.9127\n",
      "Epoch 300/300\n",
      "101/101 [==============================] - 0s 2ms/step - loss: 9.2753 - mae: 2.1176 - val_loss: 12.9813 - val_mae: 2.5712\n"
     ]
    }
   ],
   "source": [
    "#now we will train our model so we need to call the function\n",
    "model = pricepredictionmodel()\n",
    "#The batch size is a number of samples processed before the model is updated.\n",
    "#verbose is the choice that how you want to see the output of your Nural Network while it's training. If you set verbose = 0, It will show nothing\n",
    "model_history = model.fit(x_train, y_train, epochs = 300, batch_size=4, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "240aba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.65503184, 28.0934953 , 18.02901829, 21.47671576, 18.8254387 ,\n",
       "       19.87997758, 32.42014863, 18.06597765, 24.42277848, 27.00977832,\n",
       "       27.04081017, 28.75196794, 21.15677699, 26.85200196, 23.38835945,\n",
       "       20.66241266, 17.33082198, 38.24813601, 30.50550873,  8.74436733,\n",
       "       20.80203902, 16.26328126, 25.21805656, 24.85175752, 31.384365  ,\n",
       "       10.71311063, 13.80434635, 16.65930389, 36.52625779, 14.66750528,\n",
       "       21.12114902, 13.95558618, 43.16210242, 17.97539649, 21.80116017,\n",
       "       20.58294808, 17.59938821, 27.2212319 ,  9.46139365, 19.82963781,\n",
       "       24.30751863, 21.18528812, 29.57235682, 16.3431752 , 19.31483171,\n",
       "       14.56343172, 39.20885479, 18.10887551, 25.91223267, 20.33018802,\n",
       "       25.16282007, 24.42921237, 25.07123258, 26.6603279 ,  4.56151258,\n",
       "       24.0818735 , 10.88682673, 26.88926656, 16.85598381, 35.88704363,\n",
       "       19.55733853, 27.51928921, 16.58436103, 18.77551029, 11.13872875,\n",
       "       32.36392607, 36.72833773, 21.95924582, 24.57949647, 25.14868695,\n",
       "       23.42841301,  6.90732017, 16.56298149, 20.41940517, 20.80403418,\n",
       "       21.54219598, 33.85383463, 27.94645899, 25.17281456, 34.65883942,\n",
       "       18.62487738, 23.97375565, 34.6419296 , 13.34754896, 20.71097982,\n",
       "       30.0803549 , 17.13421671, 24.30528434, 19.25576671, 16.98006722,\n",
       "       27.00622638, 41.85509074, 14.11131512, 23.25736073, 14.66302672,\n",
       "       21.86977175, 23.02527624, 29.0899182 , 37.11937872, 20.53271022,\n",
       "       17.36840034, 17.71399314])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84bafc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 404, 13) for input KerasTensor(type_spec=TensorSpec(shape=(None, 404, 13), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 13).\n",
      "Predicted Output:  [[32.870003 ]\n",
      " [24.647217 ]\n",
      " [17.757113 ]\n",
      " [21.420647 ]\n",
      " [23.380007 ]\n",
      " [18.028732 ]\n",
      " [27.795288 ]\n",
      " [16.96655  ]\n",
      " [21.10694  ]\n",
      " [22.332844 ]\n",
      " [23.213398 ]\n",
      " [27.075108 ]\n",
      " [19.552698 ]\n",
      " [20.885363 ]\n",
      " [19.676361 ]\n",
      " [27.4314   ]\n",
      " [11.296062 ]\n",
      " [35.086025 ]\n",
      " [25.637241 ]\n",
      " [13.296141 ]\n",
      " [19.593018 ]\n",
      " [19.174843 ]\n",
      " [20.846092 ]\n",
      " [24.371464 ]\n",
      " [28.443548 ]\n",
      " [ 9.280016 ]\n",
      " [11.261005 ]\n",
      " [18.665699 ]\n",
      " [38.197502 ]\n",
      " [12.908378 ]\n",
      " [20.756596 ]\n",
      " [15.358311 ]\n",
      " [48.45234  ]\n",
      " [13.998262 ]\n",
      " [19.342587 ]\n",
      " [20.040241 ]\n",
      " [15.056503 ]\n",
      " [31.759335 ]\n",
      " [ 8.198465 ]\n",
      " [19.578903 ]\n",
      " [24.405144 ]\n",
      " [21.836128 ]\n",
      " [25.73785  ]\n",
      " [13.2579155]\n",
      " [13.514269 ]\n",
      " [10.403155 ]\n",
      " [51.986423 ]\n",
      " [13.116348 ]\n",
      " [22.075624 ]\n",
      " [17.253487 ]\n",
      " [21.788738 ]\n",
      " [20.10707  ]\n",
      " [29.636501 ]\n",
      " [20.22059  ]\n",
      " [12.214027 ]\n",
      " [22.363625 ]\n",
      " [12.307425 ]\n",
      " [24.97953  ]\n",
      " [17.263039 ]\n",
      " [34.559025 ]\n",
      " [18.08348  ]\n",
      " [24.185822 ]\n",
      " [13.551309 ]\n",
      " [12.005114 ]\n",
      " [20.020864 ]\n",
      " [45.860847 ]\n",
      " [37.53257  ]\n",
      " [22.395168 ]\n",
      " [22.545536 ]\n",
      " [20.150284 ]\n",
      " [24.065586 ]\n",
      " [ 6.4692   ]\n",
      " [16.881857 ]\n",
      " [18.338434 ]\n",
      " [18.653461 ]\n",
      " [21.33068  ]\n",
      " [48.146072 ]\n",
      " [22.79226  ]\n",
      " [26.976913 ]\n",
      " [30.85754  ]\n",
      " [17.277206 ]\n",
      " [21.068245 ]\n",
      " [32.073227 ]\n",
      " [15.101889 ]\n",
      " [22.023556 ]\n",
      " [25.833885 ]\n",
      " [13.6300955]\n",
      " [26.707203 ]\n",
      " [19.364603 ]\n",
      " [16.125336 ]\n",
      " [22.517431 ]\n",
      " [52.75597  ]\n",
      " [13.497113 ]\n",
      " [21.281174 ]\n",
      " [10.051385 ]\n",
      " [20.414175 ]\n",
      " [19.957722 ]\n",
      " [22.43864  ]\n",
      " [35.207626 ]\n",
      " [19.331333 ]\n",
      " [11.0621605]\n",
      " [12.667425 ]]\n"
     ]
    }
   ],
   "source": [
    "#training is complited \n",
    "#a = y_pred\n",
    "dnn_y_pred = model.predict(x_test)\n",
    "print(\"Predicted Output: \", dnn_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607e8c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  12.981302511211538\n",
      "Accuracy of model:  87.01869748878846 %\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, dnn_y_pred)\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Accuracy of model: \", 100-mse,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
